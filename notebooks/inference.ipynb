{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n"
     ]
    }
   ],
   "source": [
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForeground,\n",
    "    CropForegroundd,\n",
    "    LoadImage,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacing,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRange,\n",
    "    Orientationd,\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        EnsureChannelFirstd(keys='image'),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=['image'], source_key=\"image\"),\n",
    "        Orientationd(keys=['image'], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=['image'], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "post_processing = Compose(\n",
    "    [\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=0.0,\n",
    "            a_max=1.0,\n",
    "            b_min=-1000.0,\n",
    "            b_max=2976.0,\n",
    "        ),\n",
    "        Orientationd(keys=['image'], axcodes=\"RAS\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image to use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_path = '../Prostate Dataset/Prostate-AEC-001/11-17-1992-NA-RX SIMULATION-82988/0.000000-Contouring-60430/1-1.dcm'\n",
    "dicom_dir = '../Prostate Dataset/Prostate-AEC-001/11-17-1992-NA-RX SIMULATION-82988/2.000000-Pelvis-13578'\n",
    "weights_path = '../best_metric_model.pth'\n",
    "output_path = '../mask.dcm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "colorsD = {\n",
    "        'red': [255, 0, 0],\n",
    "        'green': [0, 255, 0],\n",
    "        'blue': [0, 0, 255],\n",
    "        'yellow': [255, 255, 0],\n",
    "        'purple': [128, 0, 128]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from monai.transforms import Compose, EnsureChannelFirstd, ScaleIntensityRanged, CropForegroundd, Orientationd, Spacingd\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "import SimpleITK as sitk\n",
    "from rt_utils import RTStructBuilder\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "\n",
    "def intialize_model(weights_path):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2,\n",
    "        norm=Norm.BATCH,\n",
    "    ).to(device)\n",
    "\n",
    "    # Load saved weights\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "\n",
    "def create_rtstruct(dicom_dir, output_path, mask_array, roi_name, roi_color):\n",
    "    \n",
    "    # Create new RT Struct. Requires the DICOM series path for the RT Struct.\n",
    "    rtstruct = RTStructBuilder.create_new(dicom_series_path= dicom_dir)\n",
    "   \n",
    "    rtstruct.add_roi(\n",
    "        mask= mask_array, \n",
    "        color= colorsD[roi_color], \n",
    "        name= roi_name\n",
    "    )\n",
    "    rtstruct.save(output_path)\n",
    "\n",
    "\n",
    "def read_dicom(dicom_dir):\n",
    "    data = {'image': dicom_dir}\n",
    "    data = LoadImaged(keys='image')(data)\n",
    "    \n",
    "    preprocessed_data = val_transforms(data) # PreProcess\n",
    "    \n",
    "    preprocessed_data = preprocessed_data['image'].unsqueeze(0) # Add Batch\n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(dicom_dir, weights_path, output_path):\n",
    "    \n",
    "    model, device = intialize_model(weights_path)\n",
    "    \n",
    "    image_dict = read_dicom(dicom_dir) # {image: metaTensor, .meta for metadata, .applied_operations for transformations history}\n",
    "    \n",
    "    # Perform sliding window inference on the single image\n",
    "    roi_size = (96, 96, 96)\n",
    "    sw_batch_size = 4 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = sliding_window_inference(image_dict['image'].to(device), roi_size, sw_batch_size, model)\n",
    "\n",
    "    \n",
    "    predicted_output = torch.argmax(output, dim=1).detach().cpu().numpy().astype(bool)\n",
    "\n",
    "    \n",
    "    # Create and save the RTStruct\n",
    "    create_rtstruct(dicom_dir, output_path, predicted_output.squeeze(0), \"Prostate\", \"red\")\n",
    "    \n",
    "    return predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 298, 276, 179])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = read_dicom(dicom_dir)\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m slice_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslice_num\u001b[38;5;241m+\u001b[39m\u001b[43mi\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,:, :, slice_num\u001b[38;5;241m+\u001b[39mi], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "slice_num = 70\n",
    "\n",
    "plt.figure(\"check\")\n",
    "plt.title(f\"image {slice_num+i}\")\n",
    "plt.imshow(image.numpy()[0,0,:, :, slice_num+i], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(\"check\")\n",
    "# plt.title(f\"image {slice_num+i}\")\n",
    "# plt.imshow(output.numpy()[0,slice_num+i, :, :], cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = preprocessing.inverse(image.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 179, 1024, 1024])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
